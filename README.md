# Архитектура интерпретатора командной строки Ezh
## Ezh, View и Environment
`Ezh` &mdash; название программы, а также ее самый верхнеуровневый класс. Он является точкой входа, отвечает за хранение и последовательный запуск классов следующего уровня. Соответственно, обрабатывает те исключения, которые не были обработаны используемыми им классами (например, исполняемые команды исключения не выбрасывают, для сообщения ошибок используют специальный поток и коды возврата). Его главный публичный метод, запускающий всю программу &mdash; `main`.

`Ezh` хранит в себе два объекта: интерфейса `View` и класса `Environment`. 

Интерфейс `View` отвечает за взаимодействие приложения с пользователем, а именно за получение от него ввода (метод `getInput`) в виде потока и за запись выходного потока программы и потока ошибок (методы `writeOutput` и `writeError` соответственно). Потоки используются, как более общий вариант, чем список строк: они хорошо проявляют себя в ленивости по отношению к потенциально большому входу; а также позволяют использовать пользовательские классы-потоки для специфических задач в будущем.<br/> 
Наконец, `View` является интерфейсом по следующей причине: в будущем, возможно, захочется поддержать различные источники и точки выхода данных программы; например, чтение из файла / запись в него; получение данных от другой программы и отправка их ей; или же чтение пользовательского рукописного ввода с графического планшета, с помощью распознавания голоса и т. д. Для выполнения текущего задания будем использовать реализацию `ConsoleView`, манипулирующей с данными аналогично оригинальному Bash &mdash; через stdin, stdout и stderr. Для этого у нее есть дополнительные методы для переопределения источников этих потоков (`setStdErr` и другие).<br/>
Важно заметить, что исключения, возникающие при манипуляциях с потоками `View`, должен обрабатывать вышестоящий `Ezh` (так как считаем их ошибками приложения в целом).

Объект класса `Environment` является же хранилищем контекста сессии, т. е. глобальных перемен. Хранение и интерфейс интуитивны: переменные хранятся в изменяемой `MutableMap`, действующей из `WORD` в `WORD`, где `WORD` &mdash; это класс токена слова, хранящий внутри строку, про них подробнее см. далее (после экспериментов с командной строкой Bash было выявлено, что в переменных хранятся именно строки, поэтому мы приняли аналогичное логичное решение). Доступ к ней и модификация происходят через методы `getVariable` и `putVariable`. Исключения данные методы не вызывают (так как чтение отсутствующей ранее переменной &mdash; это пустая строка аналогично поведению Bash-а), количество глобальных переменных отдельно кроме как размером памяти никак не ограничивается.

## Lexer: lex и postprocess

Наконец, перейдем к непосредственной работе с вводом пользователя, который `Ezh` получил в виде потока от `View`. Сначала `Ezh` передает поток с пользовательским вводом классу `Lexer` в метод `lex`.<br/> 
Цель данного класса в соответствии с его названием: разобраться со специальными символами (кавычки, равенство, пайпы), с логикой подстановок, с правильным разбиением на лексемы и в результате вернуть список токенов. При этом важно отметить, что `Lexer` никак не пользуется и не проверяет смысл написанного, этим занимается `Parser` (см. далее). Про класс `Lexer` же стоит добавить, что все его методы &mdash; статические, и сам класс, таким образом, нигде не хранится и является простой абстракцией для инкапсуляции определенных методов и логики.

Вернемся к методу `lex`. Его цель: определить конец ввода пользователя (так как в будущем это могут быть не просто спец. символы вроде переноса строки, но и специальные ключевые слова и т. д.), разобраться с кавычками (двойными и одинарными), знаками равенства и пайпов, и самое главное &mdash; выделить подстановки. Глобальная идея: чтобы подстановки не могли происходить бесконечно, необходимо два этапа разбиения на лексемы; первый &mdash; совершит подстановки, второму же будет запрещено это делать. Подобная идея решает проблему, кроме того, проявляется и в оригинальном Bash, поэтому мы пошли по такому пути. Собственно говоря, чтобы сделать подстановки &mdash; необходимо правильно разобрать кавычки (так как в одних подстановки работают, а в других &mdash; нет). При разборе же кавычек одновременно удобно выделять из пользовательского ввода токены знаков равенств и пайпов: они как и подстановки зависят от контекста кавычек. Причем, важно! Эксперименты с оригинальным Bash показали, что специальные символы (равенства, пайпов и т. д.) возникшие из подстановок, специального значения далее не имеют и интерпретируются шеллом как строки (возможно, команды). Тогда разбор равенств и пайпов на первом этапе лексического разбора кроме своего удобства влечет близость к поведению Bash-а &mdash; а значит, получаем отлично обоснованное разделение обязанностей между этапами.

Еще более подробно про `lex`. Как описывалось выше, это первый этап лексического разбора. Его задача разбить строку на следующие токены, избавляясь от символов `$=|’”` в специальных значениях:
* `WORD` &mdash; описывает строку, которая в дальнейшем будет интерпретироваться как одно слово, даже при наличии пробелов внутри (т. е., например, как один аргумент команды); последнего можно добиться, использовав пробелы внутри кавычек &mdash; такое выражение под кавычками важно интерпретировать как единое целое, аналогично поведению Bash-а (оно позволяет писать больше вариантов аргументов команд и т. д.);
* `SPACE` &mdash; токен пробельного символа (или их последовательности), нужен для разграничения слов;
* `PIPE` &mdash; токен пайпа;
* `ASSIGN` &mdash; токен знака равенства в значении присваивания (т. е. знак равенства под кавычками, например, таким не является аналогично поведению Bash-а);
* `SUBST` &mdash; имя переменной, вместо которого необходимо подставить ее значение; причем переменная находится не в окружении кавычек (это влияет на интерпретация пробелов после подстановки, см. далее);
* `QSUBST` &mdash; аналог `SUBST`, но уже в окружении двойных кавычек (в данном случае пробелы в подстановке не будут разбивать подставленную строку по пробелам).
Токены представляются классом `Token`, каждый из них в зависимости от подтипа хранит необходимую часть строчки. С помощью средств Kotlin-а их можно будет перебирать подобно enum-ам.<br/>
Автомат с его принципом работы изображен на слайде под названием *Lexer*. Основными являются следующие идеи:
* все, что находится внутри одинарных кавычек, специального значения иметь не может, при этом в итоге является одним словом или его частью;
* с двойными кавычками аналогично, однако добавляется возможность подстановок &mdash; их нужно аккуратно выделять, обрабатывая места, где заканчиваются имена переменных (они происходят по спецсимволам);
* имена переменных всегда начинают новые токены, даже там, где в итоге всего процесса лексического разбора разделения на токены не будет (пример: `text”$var”`) &mdash; причиной является главная цель первого этапа, а именно, выделить переменные для подстановки в отдельные токены;
* из-за требования в пункте выше возникают иногда непросто записываемые переходы: например, в `text“$var”` на долларе необходимо закончить идущий токен `WORD`, в случае же просто `“$var”` предыдущего токена нет, поэтому и закрывать его не надо (в схеме это отмечено как `WORD?`);
* EOF (в смысле конца пользовательского ввода, возможно, по решению `Lexer`-а &mdash; в данной реализации планируется в этом смысле интерпретировать перенос строки) может вести либо к ошибке, либо к успеху первого этапа.
Некоторые детали станут понятнее в описании следующего этапа. Итого, в результате `lex`-а `Ezh` получает список лексем, в котором выделены специальные токены, требующие подстановки, а также проинтерпретированы в лексемы все символы со специальными значениями. Далее он передает этот список на второй этап лексического разбора, в метод `postprocess` `Lexer`-у.

Основными задачами `postprocess`-а являются: 
* подстановка значений переменных вместо соответствующих токенов `QSUBST` и `SUBST`;
* полное избавление от токенов SPACE.
Задачей первого этапа было выделение подстановок и избавление от символов в спец. значениях, тогда оставшиеся задачи непосредственно подстановки и дальнейшего разбора лексем логично относятся ко второму этапу, т. е. `postprocess`-у. Схема его действий изображена на слайде *Lexer Post Processing*. 
* Сначала он выполняет подстановку в переменные токенов `QSUBST`, заменяя их на токен `WORD` со значением строки, хранимой в переменной. Значение переменных же при подстановке `Lexer` получает у `Environment`-а с глобальным контекстом, который был передан в метод `Ezh`-ом. Замечание: данная подстановка не требует дальнейших разбиений, так как `QSUBST` означает нахождение в контексте двойных кавычек, где слово по пробелам не разбивается.
* Затем аналогичным образом выполняется замена `SUBST` на строки-значения их переменных. Однако в данном случае строки-значения проходят процесс разбиения на `WORD`-ы и `SPACE`-ы, на которые уже в реальности заменяется `SUBST`. Данное отличие вызвано тем, что при подстановке вне кавычек пробелы подставляемой строки интерпретируются как разделители разных слов в Bash.
* Наконец, необходимо грамотно избавиться от `SPACE` токенов &mdash; `Parser`-у служебные символы ни к чему, он работает на уровне команд, выражений и аргументов. Перед избавлением от пробелов необходимо проверить синтаксические ошибки, с ними связанные &mdash; в данном случае это только пробелы вокруг знаков равенств, они запрещены Bash-ом и, как следствие, и нами. Наконец, перед избавлением от `SPACE`-ов необходимо соединить слова, между которыми пробелов нет &mdash; в соответствии с нашим автоматом, они могли взяться только как части выражений внутри кавычек, которые, как говорилось выше, хотим интерпретировать как одно слово. Именно здесь и становится понятной надобность `SPACE`-ов: они позволяют наглядным образом отличать ситуацию различных слов, от еще не соединенных.<br/>
Небольшой пример: 
  ```
  wc filena“$x$y”
  
  do lex:
  WORD (wc), SPACE, WORD (filena), QSUBST(x), QSUBST(y)
  
  do postprocess-1:
  WORD (wc), SPACE, WORD (filena), WORD(m), WORD(e)
  
  do postprocess-2:
  WORD (wc), WORD (filename)
  ```
Наконец, стоит добавить, что все порожденные `Lexer`-ом исключения обрабатывает `Ezh`.

В итоге лексического анализа `Ezh` получает список лексем, причем каждая из которых &mdash; либо `ASSIGN`, либо `PIPE`, либо `WORD`. Т. е. впереди предстоит преобразование этих токенов в непосредственные операции нашего терминала в смысле именно значения данных слов и операторов. Для этого `Ezh` передает полученный список лексем `Parser`-у в метод `parse`.

## Parser: parse

Автомат, описывающий принцип работы `parse`, представлен на слайде *Parser*. Идеи следующие:
* необходимо превратить токены в операции, которые можно будет как-то выполнять &mdash; их будет описывать класс `Operation` (базовый класс, ничего в себе не хранящий);
* в данном задании мы выделяем три типа операций: присваивание (класс `Assignment`, хранящий токены левой и правой части присваивания), команда (класс `Command`, хранящий список своих аргументов) и, наконец, команда завершения приложения exit (класс `Exit`, хранящий свой единственный аргумент &mdash; код возврата); причина выделения последней команды в отдельный класс будет объяснена далее, все дело в ее запуске;
* после многочисленных экспериментов с Bash было выявлено, что присваивания переменных в пайпах работают неочевидным образом (скорее, не работают), а несколько операций подряд, разделенных пробелами, также работают неинтуитивно &mdash; поэтому мы решили исправить это в своей версии языка; а именно, разрешили только следующие структуры операций:
   * единственная операция без пайпов (соответствует размеру списка токенов длины 1);
   * несколько операций, обязательно разделенных пайпами; т. е. `<Operation> <PIPE> <Operation> <PIPE> <Operation>` и т. д.
* в момент, когда `Parser` в `parse` определил, что идет вызов команды или exit (это соответствует случаю, когда было считано подряд более одного `WORD` или когда после `WORD` случился `PIPE` или конец ввода), первый токен `WORD` в последовательности он принимает за имя команды, оставшиеся &mdash; за ее аргументы; после чего перед ним стоит задача &mdash; превратить имя команды в ее конкретный класс, наследник класса `Command` (или класс `Exit`);
* наследники класса `Command` бывают двух типов: реализованные нашим приложением команды (`PwdCommand` для pwd, `EchoCommand` для echo и так далее) и остальные, которые предполагаются внешними, им соответствует класс `ExternalCommand`; никаких дополнительных полей у всех перечисленных наследников нет, однако разделение необходимо по следующей причине: команды, реализуемые в нашем приложении, важно отличать от их синонимичных по названию аналогов, реализуемых во внешней среде исполнения, после чего вызывать в первую очередь именно реализованные нами версии;
* чтобы `Parser` мог определять конкретные классы-наследники по названиям команд, необходимо где-то эту информацию иметь &mdash; для этого `Parser` хранит неизменяемую `Map` (настраивается вручную в коде) из `Token`-а имени команды в функциональный интерфейс `OperationSupplier`, который ссылается на конструктор соответствующего класса; 
  * такого вида хеш-таблица позволит не хранить зазря неиспользуемые объекты, причем по необходимости их создавать; 
  * `OperationSupplier` же был использован вместо `Supplier<Operation>` для более явного названия основного метода и гибкости в выборе аргументов; 
  * наконец, для имени команды используется тип `Token`, а не `WORD` для расширяемости &mdash; в таком случае, например, возможно перегрузить значение некоторого оператора другой командой;
* операциям присваивания `Assignment` соответствует последовательность токенов `WORD, ASSIGN, WORD`, как и представлено в оригинальном Bash-е.
Конечно, все ошибки при парсинге будет обрабатывать вышестоящий `Ezh`.

В результате получаем список из `Operation`, которые дальше предстоит последовательно запускать: такой смысл мы закладываем в пайп (напоминание: несколько операций в нашем языке разрешены только при разделении через пайпы). Для этого `Ezh` передает полученный список и контекст с глобальными переменными в метод `execute` класса `Executor`. Он как и `Lexer`, и `Parser` является классом, у которого нет состояния, а есть только статические методы &mdash; смысл в инкапсуляции логики.

## Executor: execute

Как говорилось выше, цель `execute` &mdash; последовательно выполнить операции из переданного списка. Для выполнения соответствующей операции у нее вызывается метод `doAssign` в случае `Assignment` и `execute` в случае `Command`. В случае же `Exit` целью `Executor` является завершение всей программы с требуемым кодом возврата. Идеи работы `execute` следующие:
* операции исполняются в однопоточном последовательном пайпе &mdash; однако потоки ввода, вывода и ошибок должны передаваться между ними аналогично поведению Bash-а; это позволит не только реализовать последовательное перенаправление вывода в ввод следующей операции в пайпе и логгирование, но и удовлетворить интерфейсу стандартных программ GNU, что может быть полезно;
* поддерживаем одну из основных идей передачи кодов возврата от операции к операции в пайпе;
* после долгих экспериментов с Bash мы пришли к выводу, что присваивания переменных в пайпах работают неинтуитивно (и на деле вообще не работают) &mdash; в нашей же программе мы выбрали наглядную логику, а именно: каждое присваивание в пайпе влияет только на локальный контекст, передаваемый как объект класса `Environment` от операции к операции в пайпе;
  * после цепочки пайпов локальные изменения контекста не добавляются в глобальный; за счет этого мы поддержали полезную возможность изменения контекста только в рамках одного пайпа;
  * при этом иметь возможность менять глобальный контекст &mdash; необходимо, для этого присваивание должно быть написано единственной командой без пайпа;
  * таким образом, мы поддержали изменение как локального, так и глобального контекстов;
Из идей работы следуют и сигнатуры методов `doAssign` и `execute`. Замечание: `File` используется для а) использования стандартного `ProcessBuilder` в случае запуска внешних программ б) реализации передачи потоков между операциями пайпа через временные файлы.<br/> 
Алгоритм же `execute` следующий:
* если операция единственная и это `Assignment`, то записать/обновить значение переменной в глобальной `Environment`, которую передал в execute `Ezh`; перейти к последнему пункту;
* иначе создать новый объект `Environment`, как копию глобального контекста; создать потоки `in`, `out` и `err`; двигаться последовательно по операциям;
* если операция присваивания &mdash; то обновить контекст и продолжить далее;
* если `Exit` &mdash; перейти к последнему пункту, после чего бросить специальное исключение, сигнализирующее `Ezh` о необходимости завершить программу;
* если команда &mdash; то исполнить ее `execute`, после чего в качестве нового `in` принять старый `out`, а для `out` завести новый поток; запомнить ее код ошибки, чтобы передать дальше &mdash такой подход является реализацией идеи пайпа;
  * новый поток &mdash; имеется в виду новый / второй сейчас неиспользуемый временный файл (понятное дело, на весь пайп достаточно двух временных файлов);
* последний пункт: вернуть код ошибки последней операции, `out` и `err` потоки.

После чего, наконец, `Ezh`-у останется полученные в результате исполнения потоки передать на запись `View`. После чего при отсутствии исключений потворить работу по циклу.
